{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset\n",
    "The dataset contains Amazon US Book Reviews over the span of two decades from 1995 to present. This makes Amazon Customer Reviews a rich source of information for academic researchers in the fields of Natural Language Processing (NLP), Information Retrieval (IR), and Machine Learning (ML), amongst others.\n",
    "\n",
    "My prime objective is to use a portion of the dataset to analyze recent reviews on books and cluster the customers based on sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon_reviews_us_Books_v1_02.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Description\n",
    "The following functions are carried out to get an overview of the dataset. Since the missing values are quite less, it has been decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peeping at the datset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting random rows to peep at the dataset unbiased\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical overview of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Info about the attributes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out the missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming the drop\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The new shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Conducting Exploratory Data Analysis on the dataset to derive insights. It has been observed that most of the customers has given a 5 star rating to the product and the ratio of good to bad rating rating is quite high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Univartiate Analysis to determine the ratings distribution\n",
    "df['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the ratings\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(data=df, y='star_rating', palette='dark')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['star_rating'].value_counts(), labels = df['star_rating'].value_counts().index, autopct = '%.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing review_date to Datetime format and extracting the year\n",
    "df[\"review_date\"] = pd.to_datetime(df[\"review_date\"])\n",
    "df[\"review_year\"] = df[\"review_date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bivariate analysis - distribution of number of votes with respect to review year\n",
    "print(df.groupby(\"star_rating\")[\"total_votes\"].mean())\n",
    "sns.lineplot(data = df, x = \"review_year\", y = \"total_votes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the dataset is observed to be quite large and we are only interested in the latest customer reviews, subsetting the dataset to drop the reviews that range from 1995 - 2000 has been deemed desirable.\n",
    "\n",
    "#### It has been observed that most 5 star rated reviews do not have a good number of votes and may be misleading in our analysis, therefore the reviews with a total vote count less than 8 has been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting df in order to only work with the recent dataset and the most accurate dataset\n",
    "df = df[(df['total_votes']>8.0) & (df['review_year'>1998])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With careful observation, I have decided that it would be within my best interest to get a column 'review' whch comprises of not only review body but also data on the overall rating on the product and the total votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df.apply(lambda row: f\"{row['star_rating']} stars with \n",
    "                        {row['total_votes']} votes. {row['review_headline']}: {row['review_body']}\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Review column through NLP techniques\n",
    "In order to prepare the review column for analysis, we perform the following tasks\n",
    "- Remove HTML tags\n",
    "- Remove punctuations\n",
    "- Remove stopwords\n",
    "- Lametize\n",
    "- represent the data as a frequency distribution of the words\n",
    "\n",
    "We have used WordCloud to effectively view the frequently used words in both kind of ratings [good = 3,4,5 and bad = 1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(r'<.*?>', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert words into small letter words\n",
    "df['review_body'] = df['review_body'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing through splitting the dataset\n",
    "df['review_body'] = df['review_body'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['review_body'] = df['review_body'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokens back into a string\n",
    "df['review_body'] = df['review_body'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "df['review_body'] = df['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(word for review in df[df['star_rating'].isin([5.0,4.0,3.0])]['review_body'] for word in review.split())\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(word for review in df[df['star_rating'].isin([1.0,2.0])]['review_body'] for word in review.split())\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer\n",
    "TF-IDF transforms the customer reviews into numerical vectors. These vectors represent the importance of terms in each document, which can then be used in tasks like clustering, sentiment analysis, or classification. This transformation allows machine learning models to work effectively with textual data by providing them with a numerical representation of the content.\n",
    "\n",
    "- Term Frequency (TF): Measures the frequency of a term in a document. A higher frequency indicates a term's significance within that document.\n",
    "- Inverse Document Frequency (IDF): Measures how unique or rare a term is across all documents. Rare terms get higher scores, reducing the weight of common words that appear in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit the vectorizer to the review data and transform it into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df['review_body'])\n",
    "\n",
    "# Convert the TF-IDF matrix into a dense array\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Print the shape of the TF-IDF array\n",
    "print(tfidf_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "PCA is used to preprocess large textual data (e.g., vectors created using TF-IDF) by reducing its dimensionality. This simplifies the data, reduces computational load, and helps to enhance the performance of clustering algorithms applied to the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA object with 200 components\n",
    "pca = PCA(n_components=200)\n",
    "\n",
    "# Fit the PCA object to the TF-IDF array and transform it into a lower-dimensional representation\n",
    "pca_array = pca.fit_transform(tfidf_array)\n",
    "\n",
    "# Print the shape of the PCA array\n",
    "print(pca_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scalar Normalization\n",
    "For clustering customer reviews or performing sentiment analysis, standardization ensures that all numerical features (such as TF-IDF scores) are on the same scale, leading to more meaningful and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "pca_array_scaled = scaler.fit_transform(pca_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "\n",
    "For clustering the customer reviews I have used three clustering algorithms, namely:\n",
    "- KMeans Clustering\n",
    "- Aggolomerative Heirarchial Clutering\n",
    "- DBSCAN (Density based Clustering Algorithm)\n",
    "\n",
    "From the analysis, it has been estimated that DBSCAN has provided the most accurate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering_quality(clustering_algorithm, X, n_clusters):\n",
    "    clustering_algorithm.n_clusters = n_clusters\n",
    "    cluster_labels = clustering_algorithm.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    db_index = davies_bouldin_score(X, cluster_labels)\n",
    "    return silhouette_avg, db_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering with Elbow Method\n",
    "kmeans = KMeans()\n",
    "visualizer = KElbowVisualizer(kmeans, k=(2, 6))\n",
    "visualizer.fit(pca_array_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, random_state = 42)\n",
    "kmeans.fit(pca_array_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss=[]\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(pca_array_scaled)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_n_clusters = visualizer.elbow_value_\n",
    "hierarchical = AgglomerativeClustering(n_clusters=hierarchical_n_clusters)\n",
    "hierarchical_quality = evaluate_clustering_quality(hierarchical, pca_array_scaled, hierarchical_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN clustering with different epsilon values\n",
    "dbscan_results = []\n",
    "for epsilon in [0.1, 0.5, 1, 2, 5]:\n",
    "    dbscan = DBSCAN(eps=epsilon)\n",
    "    cluster_labels = dbscan.fit_predict(pca_array_scaled)\n",
    "    silhouette_avg = silhouette_score(pca_array_scaled, cluster_labels)\n",
    "    db_index = davies_bouldin_score(pca_array_scaled, cluster_labels)\n",
    "    dbscan_results.append((epsilon, silhouette_avg, db_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering quality using Silhouette Score and Davies-Bouldin Index\n",
    "best_quality = max([kmeans_quality, hierarchical_quality] + dbscan_results, key=lambda x: x[1])\n",
    "\n",
    "print(\"Best Clustering Algorithm:\")\n",
    "print(\"----------------------------\")\n",
    "if best_quality == kmeans_quality:\n",
    "    print(f\"K-Means Clustering: n_clusters={kmeans_n_clusters}, Silhouette Score={best_quality[0]:.3f}, Davies-Bouldin Index={best_quality[1]:.3f}\")\n",
    "elif best_quality == hierarchical_quality:\n",
    "    print(f\"Hierarchical Clustering: n_clusters={hierarchical_n_clusters}, Silhouette Score={best_quality[0]:.3f}, Davies-Bouldin Index={best_quality[1]:.3f}\")\n",
    "else:\n",
    "    print(f\"DBSCAN Clustering: epsilon={best_quality[0]}, Silhouette Score={best_quality[1]:.3f}, Davies-Bouldin Index={best_quality[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Prospects\n",
    "\n",
    "- Dabbling in other clustering algorithms such as H-DBSCAN, GMM, Spectral and Mean-Shift Clustering\n",
    "- Using derived results in sentiment analysis of the customers"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1412891,
     "sourceId": 2342537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
